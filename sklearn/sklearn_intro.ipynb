{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing Around with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan for today:\n",
    "\n",
    " - **Part 1**: I will regurgitate some of the scikit-learn documentation at you, in ipython notebook form\n",
    " - **Part 2**: Shannon will give an example of scikit-learn in the wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics: What is scikit-learn?\n",
    "\n",
    "Scikit-learn, or `sklearn`, is a machine learning package for python. It is built on the familiar scientific python tools (numpy, scipy, matplotlib, etc.) and thus plays very nicely with data in the form of numpy arrays (a cursory googling shows people are also working on [better integrating sklearn with other popular pythonic data structures like pandas DataFrames](https://github.com/paulgb/sklearn-pandas)).\n",
    "\n",
    "This notebook is really just an aggregation of some of the things from the sklearn documentation that I thought were interesting and/or useful. The [documentation](http://scikit-learn.org/stable/documentation.html) is really quite well organized in my opinion, so if you are curious about the layout of scikit learn or whether it has specific capabilities that you're interested in, go check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## When should you use scikit-learn?\n",
    "\n",
    "When you have data with many samples that you can formulate into a learning problem. In other words, you'd like to find patterns within that data and/or predict properties of unknown data. The [documentation](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) breaks down learning problems into the following categories:\n",
    "\n",
    " - **Supervised learning**\n",
    "   \n",
    "   You have input data and corresponding properties that you'd like to learn from your data to predict the\n",
    "   properties of unknown data. This can fall into a couple different sub-categories:\n",
    "   \n",
    "    - **Classification**: When the data fall into two or more discrete categories; classifying hand-written digits\n",
    "    for example.\n",
    "    \n",
    "    - **Regression**: similar to classification, but the output is a continuous variable rather than trying to \n",
    "    solve for group membership. An example of regression problem might be predicting shoe size based on age, \n",
    "    weight, and gender\n",
    "        \n",
    " - **Unsupervised learning**\n",
    " \n",
    "   You have input data, but no corresponding labels or dependent variable. Instead of trying to use the data to \n",
    "   predict some output for unknown data, you're more interested in finding structure in the data that is not \n",
    "   immediately apparent. For example, you might be interested in \n",
    "   detecting clusters in the input data, or dimensionality reduction to identify components that introduce the most\n",
    "   variability in your data, or to aid in visualization.\n",
    "   \n",
    "### Summary\n",
    "`sklearn` may be a good fit for cases where:\n",
    "\n",
    "1. You have moderately sized data (>50 samples, but not \"big data\")\n",
    "  - For instance, the svm.SVC classifier fit method is worse than quadratic in the number of samples, so the\n",
    "    documentation recommends you limit use of this classifier to datasets with less than a few 10,000's samples.\n",
    "<img src=http://cdn.meme.am/instances/500x/47510205.jpg>\n",
    "\n",
    "2. You are interested in trying off-the-shelf machine learning algorithms to explore data with good performance and low learning curve\n",
    "  - `sklearn` implements quite a few regression, classification, and unsupervised learning methods, but is not\n",
    "     all-inclusive. The goal of `sklearn` to make popular machine learning approaches accessible rather than \n",
    "     implementing every algorithm out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks and Caveats\n",
    "\n",
    "`sklearn` is not a silver bullet for all types of data and every application. The documentation readily states that the goal of `sklearn` is to do a few things well rather than everything. A couple specific thoughts:\n",
    "\n",
    " - **Scalability**: see above. Make sure to check the documentation for the specific algorithms you want to use. There are usually notes about how the algorithm will perform for data with different numbers of samples and features.\n",
    "\n",
    " - **Hardware acceleration** --- While `sklearn` wraps many C-libraries such as libsvm and Liblinear for fast performance, it doesn't support gpus. The `sklearn` developers prioritize portability over the complexity that supporting extra hardware introduces.\n",
    " \n",
    " - **Neural networks** --- If you're interested in another extremely popular buzzword, **deep learning**, you might be better served checking out projects that focus specifically on neural networks, like [Pylearn2](http://deeplearning.net/software/pylearn2/), [PyBrain](http://pybrain.org/), or [caffe](http://caffe.berkeleyvision.org/) (developed at UC Berkeley). There is also [scikit-neuralnetwork](https://github.com/aigamedev/scikit-neuralnetwork) that aims to expand neural network capabilities while staying true to the `sklearn` API.\n",
    " \n",
    " - **Black box** --- As a non-statistician, this is a big caveat. `sklearn` is almost criminally easy to use. The difficulty of course is interpreting your results. Fortunately, the documentation for the algorithms has external links to relevant literature on which the implementations are based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Look at some Classifiers\n",
    "\n",
    "Shannon will focus on regression in part 2, so for now let's take a look at some of the classifiers available in\n",
    "`sklearn`. Fortunately, there is a great comparison between classifiers [in the documentation](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "\n",
    "The code from the above link is copied below so that you can run it in this notebook. Credit to Gael Varoquaux, Andreas Muller, and Jacques Grobler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "         \"Quadratic Discriminant Analysis\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn` Roadmap\n",
    "\n",
    "How do you decide which tool to use for your specific problem? Start with \n",
    "[this chart](http://scikit-learn.org/stable/tutorial/machine_learning_map/). The image is included below, but at the previous link, clicking on specific bubbles in the flow chart will take you to the corresponding documentation.\n",
    "\n",
    "<img src=http://scikit-learn.org/stable/_static/ml_map.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Breakout - A Classification Example with the included \"digits\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` indludes some data in the `datasets` package to try some basic examples. Let's tackle an (apparently) classic problem in machine learning: hand-written digit classification.\n",
    "\n",
    "This example is adapted from the [Quick Start](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) page in the sklearn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "digits = sklearn.datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn.datasets.base.Bunch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'data', 'target_names', 'DESCR', 'target'] \n",
      "\n",
      "images: <type 'numpy.ndarray'>\n",
      "data: <type 'numpy.ndarray'>\n",
      "target_names: <type 'numpy.ndarray'>\n",
      "DESCR: <type 'str'>\n",
      "target: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print digits.keys(), \"\\n\"\n",
    "# What is the types of the data stored in the bunch?\n",
    "for k, v in digits.iteritems(): print \"%s: %s\" %(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images:  (1797, 8, 8)\n",
      "data (n_samples, n_features):  (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print \"images: \", digits[\"images\"].shape\n",
    "print \"data (n_samples, n_features): \", digits[\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a feel for the data\n",
    "fig, ax = plt.subplots(2, 4, figsize=(13,9))\n",
    "for a in ax.ravel():\n",
    "    data_ind = int(digits[\"images\"].shape[0] * np.random.rand())\n",
    "    img = a.imshow(digits[\"images\"][data_ind,...], interpolation=\"nearest\", cmap=\"gray\")\n",
    "    a.set_title(\"Ground Truth: %s\" %(digits[\"target\"][data_ind]));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to the Roadmap to pick our classifier: SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try a support-vector classifier with the default parameter values\n",
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, split up our dataset into *train* and *test* sets\n",
    "\n",
    "For an explanation of why you should have separate train and test sets (to avoid **overfitting**), see [this useful article](http://scikit-learn.org/stable/modules/cross_validation.html) from the `sklearn` docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "test_size = 0.3      # Train on 70% of the data, test on the remaining 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits[\"data\"], digits[\"target\"], test_size=test_size)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The moment of truth: how well did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at some specific cases\n",
    "fig, ax = plt.subplots(2, 4, figsize=(13,9))\n",
    "for a in ax.ravel():\n",
    "    data_ind = int(X_test.shape[0] * np.random.rand())\n",
    "    img = X_test[data_ind,...].reshape((8, 8))\n",
    "    prediction = clf.predict(np.array(X_test[data_ind], ndmin=2))\n",
    "    actual = y_test[data_ind]\n",
    "    a.imshow(img, interpolation=\"nearest\", cmap=\"gray\");\n",
    "    a.set_title(\"Predicted: %s\\nGround Truth: %s\" %(prediction, actual));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37592592592592594"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier on the entire test set\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      1.00      0.52        22\n",
      "          1       0.32      1.00      0.49        17\n",
      "          2       0.50      1.00      0.67        23\n",
      "          3       0.79      1.00      0.88        42\n",
      "          4       0.27      1.00      0.43        17\n",
      "          5       0.06      1.00      0.12         4\n",
      "          6       1.00      0.09      0.17       372\n",
      "          7       0.35      1.00      0.52        20\n",
      "          8       0.04      1.00      0.07         2\n",
      "          9       0.39      1.00      0.56        21\n",
      "\n",
      "avg / total       0.83      0.38      0.31       540\n",
      "\n",
      "\n",
      "[[22  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 17  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 23  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 42  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0  0]\n",
      " [41 36 23 11 45 60 35 37 51 33]\n",
      " [ 0  0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "# Let's get a little more detail\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predicted = clf.predict(X_test)\n",
    "print classification_report(predicted, y_test)\n",
    "print\n",
    "print confusion_matrix(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well, that was disappointing... what now?\n",
    "\n",
    "The classifier we originally set up used the default values... we didn't even attempt to add any input of our own! Perhaps we should try adjusting the penalty term (C) and the kernel coefficient (gamma) to see if we can get better results. We can use the `cross_validation` module to help find better hyperparameters for our classifier by evaluating our data. *We still don't even really need to know what we're doing!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
